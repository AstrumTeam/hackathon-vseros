{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "path = \"/kaggle/input/200k-short-texts-for-humor-detection/\"\n",
    "path_vseros = \"/kaggle/input/vseros/\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"intfloat/multilingual-e5-large-instruct\")\n",
    "encoder = AutoModel.from_pretrained(\"intfloat/multilingual-e5-large-instruct\").to(device)\n",
    "\n",
    "def encode(sentences):\n",
    "    encoded_input = tokenizer(sentences, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = encoder(**encoded_input.to(device))\n",
    "        embeddings = model_output.pooler_output\n",
    "        embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings.to(\"cpu\")\n",
    "\n",
    "dataset = pd.read_csv(path + \"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.rename(columns={'text': 'Sentence', 'humor': 'Tag'}, inplace=True)\n",
    "dataset[\"Tag\"] = dataset[\"Tag\"].astype(int)\n",
    "dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_text = dataset['Tag']\n",
    "y_np = np.array(y_text)[:100000]\n",
    "y_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(y_np, return_counts=True)\n",
    "y_counts = dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_text = dataset['Sentence']\n",
    "\n",
    "# batch_size = 5000\n",
    "# for i in range(85000, len(x_text), batch_size):\n",
    "#     batch = x_text[i:i+batch_size]\n",
    "#     X_np = encode(batch.to_list()).numpy()\n",
    "#     np.save(f'X_emb_batch_{i}.npy', X_np)\n",
    "    \n",
    "# X_np0 = np.load(\"X_emb_batch_5000.npy\")\n",
    "# X_np1 = np.load(\"X_emb_batch_10000.npy\")\n",
    "# X_np2 = np.load(\"X_emb_batch_15000.npy\")\n",
    "# X_np3 = np.load(\"X_emb_batch_20000.npy\")\n",
    "# X_np4 = np.load(\"X_emb_batch_25000.npy\")\n",
    "# X_np5 = np.load(\"X_emb_batch_30000.npy\")\n",
    "# X_np6 = np.load(\"X_emb_batch_35000.npy\")\n",
    "# X_np7 = np.load(\"X_emb_batch_40000.npy\")\n",
    "# X_np8 = np.load(\"X_emb_batch_45000.npy\")\n",
    "# X_np9 = np.load(\"X_emb_batch_50000.npy\")\n",
    "# X_np10 = np.load(\"X_emb_batch_55000.npy\")\n",
    "# X_np11 = np.load(\"X_emb_batch_60000.npy\")\n",
    "# X_np12 = np.load(\"X_emb_batch_65000.npy\")\n",
    "# X_np13 = np.load(\"X_emb_batch_70000.npy\")\n",
    "# X_np14 = np.load(\"X_emb_batch_75000.npy\")\n",
    "# X_np15 = np.load(\"X_emb_batch_80000.npy\")\n",
    "# X_np16 = np.load(\"X_emb_batch_85000.npy\")\n",
    "# X_np17 = np.load(\"X_emb_batch_90000.npy\")\n",
    "# X_np18 = np.load(\"X_emb_batch_95000.npy\")\n",
    "# X_np19 = np.load(\"X_emb_batch_100000.npy\")\n",
    "# X_np20 = np.load(\"X_emb_batch_105000.npy\")\n",
    "# X_np21 = np.load(\"X_emb_batch_110000.npy\")\n",
    "# X_np22 = np.load(\"X_emb_batch_115000.npy\")\n",
    "# X_np23 = np.load(\"X_emb_batch_120000.npy\")\n",
    "# X_np24 = np.load(\"X_emb_batch_125000.npy\")\n",
    "# X_np25 = np.load(\"X_emb_batch_130000.npy\")\n",
    "# X_np26 = np.load(\"X_emb_batch_135000.npy\")\n",
    "# X_np27 = np.load(\"X_emb_batch_140000.npy\")\n",
    "# X_np28 = np.load(\"X_emb_batch_145000.npy\")\n",
    "# X_np29 = np.load(\"X_emb_batch_150000.npy\")\n",
    "# X_np30 = np.load(\"X_emb_batch_155000.npy\")\n",
    "# X_np31 = np.load(\"X_emb_batch_160000.npy\")\n",
    "# X_np32 = np.load(\"X_emb_batch_165000.npy\")\n",
    "# X_np33 = np.load(\"X_emb_batch_170000.npy\")\n",
    "# X_np35 = np.load(\"X_emb_batch_175000.npy\")\n",
    "# X_np35 = np.load(\"X_emb_batch_180000.npy\")\n",
    "# X_np36 = np.load(\"X_emb_batch_185000.npy\")\n",
    "# X_np37 = np.load(\"X_emb_batch_190000.npy\")\n",
    "# X_np38 = np.load(\"X_emb_batch_195000.npy\")\n",
    "# X_np39 = np.load(\"X_emb_batch_200000.npy\")\n",
    "\n",
    "# X_np = np.concatenate([\n",
    "#     X_np0, X_np1, X_np2, X_np3, X_np4, \n",
    "#     X_np5, X_np6, X_np7, X_np8, X_np9, \n",
    "#     X_np10, X_np11, X_np12, X_np13, \n",
    "#     X_np14, X_np15, X_np16, X_np17, \n",
    "#     X_np18, X_np19, X_np20\n",
    "# ])\n",
    "# print(X_np.shape)\n",
    "\n",
    "# np.save('X_np.npy', X_np)\n",
    "X_np = np.load(path_vseros + \"X_humor_np.npy\")\n",
    "X_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X_np).type(torch.float)\n",
    "y = torch.from_numpy(y_np).type(torch.float)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,random_state=42, stratify=y) \n",
    "X_train = X_train.to(device)\n",
    "X_test = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test = y_test.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificator = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1)\n",
    ")\n",
    "classificator.to(device)\n",
    "\n",
    "neg_count = y_counts[0]\n",
    "pos_count = y_counts[1]\n",
    "pos_weight = torch.tensor([neg_count / pos_count]).to(device)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "optimizer = torch.optim.SGD(params=classificator.parameters(), lr=0.02)\n",
    "\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n",
    "    acc = (correct / len(y_pred)) * 100 \n",
    "    return acc\n",
    "\n",
    "def f1_fn(y_true, y_pred):\n",
    "    return f1_score(y_true.detach().to('cpu'), y_pred.detach().to('cpu'), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_binaryclass_classification_model(cl_model, n_epochs, print_every_epoch=None, batch_size=128):\n",
    "    if print_every_epoch is None:\n",
    "        print_every_epoch = n_epochs / 10\n",
    "    \n",
    "    torch.manual_seed(42)\n",
    "    best_loss = 10000000\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "    \n",
    "        permutation = torch.randperm(X_train.size()[0])\n",
    "        \n",
    "        for i in range(0,X_train.size()[0], batch_size):\n",
    "            ### Обучение\n",
    "            cl_model.train()\n",
    "            \n",
    "            indices = permutation[i:i+batch_size]\n",
    "            x_batch, y_batch = X_train[indices], y_train[indices]\n",
    "            \n",
    "            y_logits = cl_model(x_batch).squeeze()\n",
    "            y_pred = torch.round(torch.sigmoid(y_logits))\n",
    "        \n",
    "            loss = loss_fn(y_logits, y_batch) \n",
    "\n",
    "            acc = accuracy_fn(y_batch,y_pred)\n",
    "            f1 = f1_fn(y_batch, y_pred)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        ### Тестирование\n",
    "        cl_model.eval()\n",
    "        with torch.inference_mode():\n",
    "            test_logits = cl_model(X_test).squeeze() \n",
    "            test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "            \n",
    "            test_loss = loss_fn(test_logits, y_test)\n",
    "            test_acc = accuracy_fn(y_test,test_pred)\n",
    "            test_f1 = f1_fn(y_test, test_pred)\n",
    "\n",
    "        if test_f1 > best_f1 or (test_f1 >= best_f1 and test_loss < best_loss): \n",
    "            best_f1 = test_f1\n",
    "            best_loss = test_loss\n",
    "            print(\"Save best model\")\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}%, F1: {f1:.2f} | T.Loss: {test_loss:.5f}, T.Acc: {test_acc:.2f}%, T.F1: {test_f1:.2f}\")\n",
    "            print(\"-------\")\n",
    "            torch.save(classificator.state_dict(), \"best_clf_interest_model.pth\")\n",
    "        \n",
    "        if epoch % print_every_epoch == 0:\n",
    "            print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}%, F1: {f1:.2f} | T.Loss: {test_loss:.5f}, T.Acc: {test_acc:.2f}%, T.F1: {test_f1:.2f}\") \n",
    "            \n",
    "learn_binaryclass_classification_model(classificator, 1000, 10, 4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1024),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=1024, out_features=1)\n",
    ")\n",
    "model.load_state_dict(torch.load(\"best_clf_interest_model.pth\", map_location=device, weights_only=True))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "test_logits = model(X_test).squeeze() \n",
    "test_pred = torch.round(torch.sigmoid(test_logits))\n",
    "y_test_a = y_test.detach().to('cpu')\n",
    "y_pred_a = test_pred.detach().to('cpu')\n",
    "\n",
    "print('accuracy:', round(accuracy_score(y_test_a, y_pred_a), 3))\n",
    "print('precision:', round(precision_score(y_test_a, y_pred_a), 3))\n",
    "print('recall:', round(recall_score(y_test_a, y_pred_a), 3))\n",
    "print('f1:', round(f1_score(y_test_a, y_pred_a, average='weighted'), 3))\n",
    "\n",
    "cm = confusion_matrix(y_test_a, y_pred_a, labels=[1, 0])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[1, 0])\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
